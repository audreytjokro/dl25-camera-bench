{"cells":[{"cell_type":"markdown","metadata":{"id":"z_SsgPgpI2rz"},"source":[]},{"cell_type":"markdown","metadata":{"id":"oC2BNyfvuwn-"},"source":["# Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PUw5RBXOuVqJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Install dependencies\n","!pip install -q torch torchvision transformers accelerate\n","!pip install -q \"qwen-vl-utils[decord]==0.0.8\"\n","!pip install -q xlsxwriter imageio[ffmpeg]\n","\n","# Mount Google Drive early\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import torch"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1764538246119,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"cDurRDQGK7sZ","outputId":"96c2cb39-4747-41eb-8c6f-98edf6e7a76f"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úì Torch: 2.9.0+cu126\n","‚úì CUDA: True\n","‚úì GPU: NVIDIA A100-SXM4-40GB\n","\n","üìÅ Persistent videos: /content/drive/MyDrive/CameraBench/videos_mp4\n","üìÅ Outputs: /content/drive/MyDrive/CameraBench/outputs\n"]}],"source":["# Configuration - Edit these paths as needed\n","CONFIG = {\n","    \"drive_root\": \"/content/drive/MyDrive/CameraBench\",\n","    \"local_root\": \"/content/camerabench\",\n","    \"model_id\": \"Qwen/Qwen2.5-VL-7B-Instruct\", # change here if you want to change the model\n","    # \"model_id\": \"chancharikm/qwen2.5-vl-7b-cam-motion\",\n","    \"max_frames\": 32,\n","    \"fps\": 8.0,\n","    \"download_workers\": 16,\n","    \"convert_workers\": 8,\n","    \"batch_size\": 1,  # For parallel inference\n","}\n","\n","# Create directories (both local and persistent on Drive)\n","PATHS = {\n","    \"videos_local\": f\"{CONFIG['local_root']}/videos\",\n","    \"videos_drive\": f\"{CONFIG['drive_root']}/videos_mp4\",  # Persistent storage\n","    \"outputs\": f\"{CONFIG['drive_root']}/outputs\",\n","}\n","\n","for p in PATHS.values():\n","    os.makedirs(p, exist_ok=True)\n","\n","# System info\n","print(f\"‚úì Torch: {torch.__version__}\")\n","print(f\"‚úì CUDA: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n","print(f\"\\nüìÅ Persistent videos: {PATHS['videos_drive']}\")\n","print(f\"üìÅ Outputs: {PATHS['outputs']}\")"]},{"cell_type":"markdown","metadata":{"id":"yjRZG01Xu2av"},"source":["# Utility Functions"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1764538258234,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"FUMJVtmLbDse"},"outputs":[],"source":["import hashlib\n","import requests\n","import numpy as np\n","import imageio\n","import shutil\n","from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n","from tqdm import tqdm\n","from typing import List, Dict, Optional, Tuple\n","import random\n","\n","\n","def safe_filename(url: str) -\u003e str:\n","    \"\"\"Generate safe filename from URL.\"\"\"\n","    h = hashlib.md5(url.encode()).hexdigest()[:16]\n","    base = os.path.basename(url).split(\"?\")[0]\n","    return f\"{h}_{base}\"\n","\n","\n","def download_single_file(args: Tuple[int, str, str]) -\u003e Optional[Dict]:\n","    \"\"\"Download a single file. Returns dict with metadata or None on failure.\"\"\"\n","    idx, url, path = args\n","    if os.path.exists(path) and os.path.getsize(path) \u003e 0:\n","        return {\"idx\": idx, \"url\": url, \"path\": path}\n","    try:\n","        r = requests.get(url, stream=True, timeout=30)\n","        if r.status_code == 200:\n","            with open(path, \"wb\") as f:\n","                for chunk in r.iter_content(chunk_size=1024 * 1024):\n","                    f.write(chunk)\n","            return {\"idx\": idx, \"url\": url, \"path\": path}\n","    except Exception as e:\n","        pass\n","    return None\n","\n","\n","def download_videos(dataset, output_dir: str, max_workers: int = 16) -\u003e List[Dict]:\n","    \"\"\"Download all videos from dataset in parallel.\"\"\"\n","    tasks = []\n","    for idx, row in enumerate(dataset):\n","        url = row.get(\"video\") or row.get(\"Video\")\n","        if not url:\n","            continue\n","        fname = safe_filename(url)\n","        local_path = os.path.join(output_dir, fname if \".\" in fname else fname + \".gif\")\n","        tasks.append((idx, url, local_path))\n","\n","    manifest = []\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {executor.submit(download_single_file, t): t for t in tasks}\n","        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading\"):\n","            result = future.result()\n","            if result:\n","                manifest.append(result)\n","\n","    print(f\"‚úì Downloaded: {len(manifest)} videos\")\n","    return manifest\n","\n","\n","def convert_gif_to_mp4(args: Tuple[str, str]) -\u003e Optional[str]:\n","    \"\"\"Convert GIF to MP4 with even dimensions.\"\"\"\n","    gif_path, mp4_path = args\n","\n","    if os.path.exists(mp4_path) and os.path.getsize(mp4_path) \u003e 0:\n","        return mp4_path\n","\n","    try:\n","        frames = []\n","        with imageio.get_reader(gif_path) as reader:\n","            for frame in reader:\n","                if isinstance(frame, np.ndarray):\n","                    if frame.ndim == 2:\n","                        frame = np.stack([frame] * 3, axis=-1)\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3]\n","                frames.append(frame.astype(np.uint8))\n","\n","        if not frames:\n","            return None\n","\n","        # Ensure even dimensions for codec\n","        h, w = frames[0].shape[:2]\n","        H, W = h + (h % 2), w + (w % 2)\n","        if H != h or W != w:\n","            frames = [\n","                np.pad(f, ((0, H - h), (0, W - w), (0, 0)), mode='constant')\n","                for f in frames\n","            ]\n","\n","        imageio.mimsave(mp4_path, frames, fps=8.0, codec=\"libx264\",\n","                        ffmpeg_params=[\"-pix_fmt\", \"yuv420p\"])\n","        return mp4_path if os.path.exists(mp4_path) else None\n","    except Exception:\n","        return None\n","\n","\n","def convert_videos(manifest: List[Dict], output_dir: str, max_workers: int = 8) -\u003e List[Dict]:\n","    \"\"\"Convert all GIFs to MP4 and store in output directory.\"\"\"\n","    convert_tasks = []\n","\n","    for m in manifest:\n","        base_name = os.path.basename(m[\"path\"]).rsplit(\".\", 1)[0] + \".mp4\"\n","        mp4_path = os.path.join(output_dir, base_name)\n","\n","        if m[\"path\"].lower().endswith(\".gif\"):\n","            convert_tasks.append((m, m[\"path\"], mp4_path))\n","        else:\n","            # Non-GIF: copy to output dir if not already there\n","            if not os.path.exists(mp4_path):\n","                shutil.copy2(m[\"path\"], mp4_path)\n","            m[\"mp4_path\"] = mp4_path\n","\n","    # Parallel conversion\n","    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {executor.submit(convert_gif_to_mp4, (t[1], t[2])): t[0] for t in convert_tasks}\n","        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Converting\"):\n","            m = futures[future]\n","            result = future.result()\n","            m[\"mp4_path\"] = result if result else m[\"path\"]\n","\n","    # Ensure all entries have mp4_path\n","    for m in manifest:\n","        if \"mp4_path\" not in m:\n","            base_name = os.path.basename(m[\"path\"]).rsplit(\".\", 1)[0] + \".mp4\"\n","            m[\"mp4_path\"] = os.path.join(output_dir, base_name)\n","\n","    valid = [m for m in manifest if os.path.exists(m.get(\"mp4_path\", \"\"))]\n","    print(f\"‚úì Converted: {len(valid)} videos ready\")\n","    return valid\n","\n","\n","def load_existing_manifest(video_dir: str, dataset) -\u003e List[Dict]:\n","    \"\"\"Load manifest from existing converted videos on Drive.\"\"\"\n","    manifest = []\n","\n","    # Build URL to index mapping\n","    url_to_idx = {}\n","    for idx, row in enumerate(dataset):\n","        url = row.get(\"video\") or row.get(\"Video\")\n","        if url:\n","            url_to_idx[safe_filename(url).rsplit(\".\", 1)[0]] = (idx, url)\n","\n","    # Scan existing MP4 files\n","    if os.path.exists(video_dir):\n","        for fname in os.listdir(video_dir):\n","            if fname.endswith(\".mp4\"):\n","                base = fname.rsplit(\".\", 1)[0]\n","                if base in url_to_idx:\n","                    idx, url = url_to_idx[base]\n","                    manifest.append({\n","                        \"idx\": idx,\n","                        \"url\": url,\n","                        \"mp4_path\": os.path.join(video_dir, fname)\n","                    })\n","\n","    print(f\"‚úì Found {len(manifest)} existing converted videos\")\n","    return manifest\n","\n","\n","def select_samples(manifest: List[Dict], n: Optional[int] = None, seed: int = 42) -\u003e List[Dict]:\n","    \"\"\"Randomly select n samples from manifest. If n is None, return all.\"\"\"\n","    if n is None or n \u003e= len(manifest):\n","        return manifest\n","    random.seed(seed)\n","    selected = random.sample(manifest, n)\n","    print(f\"‚úì Selected {len(selected)} random samples (seed={seed})\")\n","    return selected\n","\n","\n","def caption_single(video_path: str, prompt: str, max_frames: int = 32, fps: float = 6.0) -\u003e str:\n","    \"\"\"Generate caption for a single video.\"\"\"\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\"type\": \"video\", \"video\": video_path, \"fps\": fps, \"max_frames\": max_frames},\n","            {\"type\": \"text\", \"text\": prompt},\n","        ],\n","    }]\n","\n","    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    image_inputs, video_inputs = process_vision_info(messages)\n","\n","    inputs = processor(\n","        text=[text],\n","        images=image_inputs,\n","        videos=video_inputs,\n","        padding=True,\n","        return_tensors=\"pt\",\n","    ).to(model.device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(**inputs, max_new_tokens=256)\n","\n","    generated_ids_trimmed = [\n","        out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)\n","    ]\n","\n","    output = processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )[0]\n","\n","    return output.strip()\n","\n","\n","def caption_batch(video_paths: List[str], prompt: str, max_frames: int = 32, fps: float = 6.0) -\u003e List[str]:\n","    \"\"\"Generate captions for a batch of videos (parallel on GPU).\"\"\"\n","    if not video_paths:\n","        return []\n","\n","    messages_batch = []\n","    for video_path in video_paths:\n","        messages_batch.append([{\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"video\", \"video\": video_path, \"fps\": fps, \"max_frames\": max_frames},\n","                {\"type\": \"text\", \"text\": prompt},\n","            ],\n","        }])\n","\n","    texts = [\n","        processor.apply_chat_template(m, tokenize=False, add_generation_prompt=True)\n","        for m in messages_batch\n","    ]\n","\n","    # Process vision info for each\n","    all_image_inputs = []\n","    all_video_inputs = []\n","    for m in messages_batch:\n","        img_inp, vid_inp = process_vision_info(m)\n","        all_image_inputs.append(img_inp)\n","        all_video_inputs.append(vid_inp)\n","\n","    # Flatten video inputs\n","    flat_videos = []\n","    for v in all_video_inputs:\n","        if v:\n","            flat_videos.extend(v)\n","\n","    inputs = processor(\n","        text=texts,\n","        images=None,\n","        videos=flat_videos if flat_videos else None,\n","        padding=True,\n","        return_tensors=\"pt\",\n","    ).to(model.device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(**inputs, max_new_tokens=256)\n","\n","    generated_ids_trimmed = [\n","        out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)\n","    ]\n","\n","    outputs = processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","\n","    return [o.strip() for o in outputs]\n","\n","\n","def run_inference(\n","    manifest: List[Dict],\n","    dataset,\n","    prompts: Dict[str, str],\n","    batch_size: int = 1,\n","    max_frames: int = 32,\n","    fps: float = 6.0\n",") -\u003e List[Dict]:\n","    \"\"\"\n","    Run inference on manifest with optional batching.\n","\n","    Args:\n","        manifest: List of video metadata dicts\n","        dataset: Original dataset for labels\n","        prompts: Dict of {caption_name: prompt_text}\n","        batch_size: Number of videos to process together (1 = sequential)\n","        max_frames: Max frames per video\n","        fps: Frames per second to sample\n","\n","    Returns:\n","        List of result dicts\n","    \"\"\"\n","    results = []\n","\n","    if batch_size == 1:\n","        # Sequential processing (more stable)\n","        for m in tqdm(manifest, desc=\"Captioning\"):\n","            video_path = m[\"mp4_path\"]\n","            idx = m[\"idx\"]\n","\n","            if not os.path.exists(video_path):\n","                continue\n","\n","            try:\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","                result = {\n","                    \"row_idx\": idx,\n","                    \"video_url\": m[\"url\"],\n","                    \"labels\": dataset[idx].get(\"labels\"),\n","                    \"human_caption\": dataset[idx].get(\"caption\"),\n","                }\n","\n","                for name, prompt in prompts.items():\n","                    result[name] = caption_single(video_path, prompt, max_frames, fps)\n","\n","                results.append(result)\n","\n","            except Exception as e:\n","                print(f\"Error on idx {idx}: {e}\")\n","                continue\n","    else:\n","        # Batch processing\n","        for i in tqdm(range(0, len(manifest), batch_size), desc=\"Batch Captioning\"):\n","            batch = manifest[i:i + batch_size]\n","            valid_batch = [(m, m[\"mp4_path\"]) for m in batch if os.path.exists(m[\"mp4_path\"])]\n","\n","            if not valid_batch:\n","                continue\n","\n","            try:\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","                batch_results = [{\n","                    \"row_idx\": m[\"idx\"],\n","                    \"video_url\": m[\"url\"],\n","                    \"labels\": dataset[m[\"idx\"]].get(\"labels\"),\n","                    \"human_caption\": dataset[m[\"idx\"]].get(\"caption\"),\n","                } for m, _ in valid_batch]\n","\n","                video_paths = [vp for _, vp in valid_batch]\n","\n","                for name, prompt in prompts.items():\n","                    try:\n","                        captions = caption_batch(video_paths, prompt, max_frames, fps)\n","                        for j, cap in enumerate(captions):\n","                            batch_results[j][name] = cap\n","                    except Exception as e:\n","                        print(f\"Batch error for {name}, falling back to sequential: {e}\")\n","                        # Fallback to sequential\n","                        for j, (_, vp) in enumerate(valid_batch):\n","                            batch_results[j][name] = caption_single(vp, prompt, max_frames, fps)\n","\n","                results.extend(batch_results)\n","\n","            except Exception as e:\n","                print(f\"Batch error at {i}: {e}\")\n","                continue\n","\n","    print(f\"‚úì Completed: {len(results)} videos\")\n","    return results\n","\n","# Optional: Quick Test on Single Video\n","def test_single_video(idx: int = 0):\n","    \"\"\"Quick test on a single video.\"\"\"\n","    if idx \u003e= len(manifest):\n","        print(f\"Index {idx} out of range. Max: {len(manifest)-1}\")\n","        return\n","\n","    m = manifest[idx]\n","    print(f\"Testing video {m['idx']}: {m['url'][:50]}...\")\n","\n","    for name, prompt in PROMPTS.items():\n","        result = caption_single(m[\"mp4_path\"], prompt)\n","        print(f\"\\n{name}:\\n{result}\")"]},{"cell_type":"markdown","metadata":{"id":"KHqGuD55vCNP"},"source":["# (Optional) Download \u0026 Convert Videos\n","\n","You only need to run this once. All the transformed data will be stored in your google drive."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2911,"status":"ok","timestamp":1764538028643,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"a0qKy94zUC5P","outputId":"a75060e8-84dd-48f1-f6f8-52529f2dd8a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úì Dataset loaded: 1071 samples\n","‚úì Found 1071 existing converted videos\n","‚úì Using existing converted videos from Drive\n","\n","üìä Total videos available: 1071\n"]}],"source":["from datasets import load_dataset\n","\n","# Load dataset\n","ds = load_dataset(\"syCen/CameraBench\", split=\"test\")\n","print(f\"‚úì Dataset loaded: {len(ds)} samples\")\n","\n","# Check for existing converted videos first\n","manifest = load_existing_manifest(PATHS[\"videos_drive\"], ds)\n","\n","# If no existing videos, download and convert\n","if len(manifest) == 0:\n","    print(\"\\nüì• No existing videos found. Downloading...\")\n","    manifest = download_videos(\n","        ds,\n","        PATHS[\"videos_local\"],\n","        max_workers=CONFIG[\"download_workers\"]\n","    )\n","\n","    print(\"\\nüîÑ Converting to MP4...\")\n","    manifest = convert_videos(\n","        manifest,\n","        PATHS[\"videos_drive\"],  # Store on Drive for persistence\n","        max_workers=CONFIG[\"convert_workers\"]\n","    )\n","else:\n","    print(\"‚úì Using existing converted videos from Drive\")\n","\n","print(f\"\\nüìä Total videos available: {len(manifest)}\")"]},{"cell_type":"markdown","metadata":{"id":"vmpqL_UtwZlV"},"source":["# Load Model"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"collapsed":true,"executionInfo":{"elapsed":6590,"status":"ok","timestamp":1764538039727,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"_OkhEeHKbDk0","outputId":"3388ca42-593d-44e5-d436-652299763971"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18a18fc0bfac4378be8bb56b1cad6685","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["‚úì Model loaded: Qwen/Qwen2.5-VL-7B-Instruct\n"]}],"source":["from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n","from qwen_vl_utils import process_vision_info\n","import gc\n","\n","# Load model\n","model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n","    CONFIG[\"model_id\"],\n","    torch_dtype=\"auto\",\n","    device_map=\"auto\"\n",")\n","\n","processor = AutoProcessor.from_pretrained(CONFIG[\"model_id\"])\n","processor.tokenizer.padding_side = \"left\" # for batch inference\n","\n","print(f\"‚úì Model loaded: {CONFIG['model_id']}\")"]},{"cell_type":"markdown","metadata":{"id":"LJTekWx2xJbj"},"source":["# Run Inference"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1764538290897,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"TjN6l8bgUXAs"},"outputs":[],"source":["CURRICULUM_ICL_PROMPT = \"\"\"\n","You are an expert video analyst. You must describe BOTH:\n","\n","1. How the CAMERA moves (camera motion).\n","2. What the SCENE contains (objects, people, setting, actions).\n","\n","You will ALWAYS output EXACTLY TWO lines in this format:\n","\n","CAMERA_MOTION: \u003cone short sentence only about camera movement\u003e\n","SCENE_DESCRIPTION: \u003cone or two short sentences only about scene content\u003e\n","\n","Important rules:\n","- CAMERA_MOTION must ONLY describe camera motion (pan, tilt, zoom, dolly, truck/slide, roll, orbit, handheld shake, static).\n","- SCENE_DESCRIPTION must ONLY describe visible scene content (subjects, environment, actions); never mention camera movement.\n","- Do NOT mention ‚Äúvideo‚Äù, ‚Äúframes‚Äù, ‚Äúshot types‚Äù, or technical terms like ‚ÄúFOV‚Äù.\n","- Keep both lines concise and natural.\n","- Never output anything except the two required lines.\n","\n","ICL Strategy (Curriculum Learning):\n","You will be shown SIX EXAMPLES ordered from simple ‚Üí medium ‚Üí complex in terms of camera motion.\n","These examples highlight progressively more difficult motion patterns.\n","Use the style, clarity, and separation demonstrated in the examples to guide your final output.\n","\n","EXAMPLES\n","========\n","Example 1 (simple: static)\n","CAMERA_MOTION: The camera remains almost completely still on a tripod with only a slight natural sway.\n","SCENE_DESCRIPTION: A person stands on a small indoor stage speaking to an audience seated in rows.\n","\n","Example 2 (simple-medium: slow pan)\n","CAMERA_MOTION: The camera slowly pans from left to right in a smooth, steady motion.\n","SCENE_DESCRIPTION: A city skyline with tall buildings and a river appears under a warm sunset glow.\n","\n","Example 3 (medium: handheld forward)\n","CAMERA_MOTION: A handheld camera walks forward with small side-to-side shakes.\n","SCENE_DESCRIPTION: Someone moves through a crowded outdoor market lined with colorful food stalls and pedestrians.\n","\n","Example 4 (medium-hard: zoom-in)\n","CAMERA_MOTION: The camera gently zooms in toward the subject while maintaining center alignment.\n","SCENE_DESCRIPTION: A person sits at a desk in a dim room illuminated by a computer monitor and soft ambient light.\n","\n","Example 5 (hard: orbit)\n","CAMERA_MOTION: The camera slowly orbits counterclockwise around the main subject.\n","SCENE_DESCRIPTION: A skateboarder performs tricks in a lively skatepark surrounded by graffiti-covered walls.\n","\n","Example 6 (hard: tracking)\n","CAMERA_MOTION: The camera quickly tracks alongside a moving subject, then eases to a stop.\n","SCENE_DESCRIPTION: A runner moves along a forest trail lined with tall trees before slowing near a bright clearing.\n","\n","Now describe the current video using ONLY the two-line format:\n","\n","CAMERA_MOTION:\n","SCENE_DESCRIPTION:\n","\"\"\"\n","\n","\n","CONTRASTIVE_ICL_PROMPT = \"\"\"\n","You are an expert video analyst. You must describe BOTH:\n","\n","1. How the CAMERA moves (camera motion).\n","2. What the SCENE contains (objects, people, setting, actions).\n","\n","You will ALWAYS output EXACTLY TWO lines in this format:\n","\n","CAMERA_MOTION: \u003cone short sentence only about camera movement\u003e\n","SCENE_DESCRIPTION: \u003cone or two short sentences only about scene content\u003e\n","\n","Important rules:\n","- CAMERA_MOTION must ONLY describe how the camera moves (pan, tilt, zoom, dolly, truck/slide, roll, orbit, handheld, static).\n","- SCENE_DESCRIPTION must ONLY describe visible scene content.\n","- Strictly avoid mixing the two categories.\n","- Do NOT mention ‚Äúvideo‚Äù, ‚Äúframes‚Äù, or technical shot terminology.\n","- Output must be concise, natural, and exactly two lines.\n","\n","ICL Strategy (Contrastive):\n","You will see SIX EXAMPLES: four GOOD examples that correctly separate camera and scene, and two BAD examples that incorrectly mix them.\n","Learn from the GOOD examples only; the BAD examples show mistakes to avoid.\n","\n","EXAMPLES\n","========\n","\n","GOOD Example 1 (static)\n","CAMERA_MOTION: The camera remains fixed with only a slight natural sway.\n","SCENE_DESCRIPTION: A speaker stands on a small stage addressing an audience seated in rows.\n","\n","GOOD Example 2 (pan)\n","CAMERA_MOTION: The camera slowly pans from left to right in a smooth motion.\n","SCENE_DESCRIPTION: A city skyline with tall buildings and a river appears under the evening sky.\n","\n","GOOD Example 3 (handheld)\n","CAMERA_MOTION: A handheld camera advances forward with gentle side-to-side shakes.\n","SCENE_DESCRIPTION: A crowded market street is filled with food stalls, signs, and pedestrians.\n","\n","GOOD Example 4 (orbit)\n","CAMERA_MOTION: The camera gently orbits around the subject in a half circle.\n","SCENE_DESCRIPTION: A skateboarder practices tricks in a bright skatepark surrounded by graffiti.\n","\n","BAD Example 5 (incorrect mixing: scene in camera)\n","CAMERA_MOTION: A person walks through a narrow alley lined with shops.   \u003c-- WRONG\n","SCENE_DESCRIPTION: The camera slowly pans across the scene.              \u003c-- WRONG\n","\n","BAD Example 6 (incorrect mixing: camera in scene)\n","CAMERA_MOTION: The camera shakes as someone moves through the trail.     \u003c-- WRONG\n","SCENE_DESCRIPTION: A shaky handheld shot follows the subject closely.    \u003c-- WRONG\n","\n","Now ignore the BAD examples and describe the current video using ONLY the two-line format:\n","\n","CAMERA_MOTION:\n","SCENE_DESCRIPTION:\n","\"\"\"\n","\n","\n","ROLE_BASED_ICL_PROMPT = \"\"\"\n","You are an expert video analyst. You must describe BOTH:\n","\n","1. How the CAMERA moves (camera motion).\n","2. What the SCENE contains (objects, people, setting, actions).\n","\n","You will ALWAYS output EXACTLY TWO lines in this format:\n","\n","CAMERA_MOTION: \u003cone short sentence only about camera movement\u003e\n","SCENE_DESCRIPTION: \u003cone or two short sentences only about scene content\u003e\n","\n","Important rules:\n","- CAMERA_MOTION must ONLY describe motion types (pan, tilt, zoom, dolly, orbit, handheld, static).\n","- SCENE_DESCRIPTION must ONLY describe scene subjects, environments, and actions.\n","- You may not reference ‚Äúvideo‚Äù, ‚Äúframes‚Äù, or technical camera terminology beyond motion.\n","- Output must be exactly two lines.\n","\n","ICL Strategy (Role-Based Decomposition):\n","Imagine TWO coordinated experts analyzing the video:\n","- The CAMERA OPERATOR describes only how the camera moves.\n","- The SCENE OBSERVER describes only what appears in the scene.\n","Their combined perspectives should guide your final output, while still using the required tags.\n","\n","EXAMPLES\n","========\n","\n","Example 1 (static)\n","CAMERA_MOTION: The camera remains almost completely still with a slight natural sway.\n","SCENE_DESCRIPTION: A person stands on a small indoor stage speaking to an audience.\n","\n","Example 2 (slow pan)\n","CAMERA_MOTION: The camera slowly pans from left to right in a smooth arc.\n","SCENE_DESCRIPTION: A skyline of tall buildings and a river comes into view at sunset.\n","\n","Example 3 (handheld)\n","CAMERA_MOTION: A handheld camera moves forward with small side-to-side shakes.\n","SCENE_DESCRIPTION: A bustling market street is lined with food stalls and pedestrians walking by.\n","\n","Example 4 (zoom)\n","CAMERA_MOTION: The camera gently zooms in on the subject while remaining centered.\n","SCENE_DESCRIPTION: A person works at a desk lit by a computer monitor in a dim room.\n","\n","Example 5 (orbit)\n","CAMERA_MOTION: The camera slowly circles around the main subject in a controlled motion.\n","SCENE_DESCRIPTION: A skateboarder practices tricks in a bright skatepark surrounded by graffiti.\n","\n","Example 6 (tracking)\n","CAMERA_MOTION: The camera quickly tracks alongside a moving subject, then slows down.\n","SCENE_DESCRIPTION: A runner moves along a forest path lined with tall trees before slowing at a clearing.\n","\n","Now describe the current video using ONLY the two-line format:\n","\n","CAMERA_MOTION:\n","SCENE_DESCRIPTION:\n","\"\"\"\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"executionInfo":{"elapsed":94259,"status":"error","timestamp":1764538388501,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"LE5A87sKLCPd","outputId":"a308f7df-2c5c-4c74-9082-9e98e3b460d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úì Selected 200 random samples (seed=42)\n"]},{"name":"stderr","output_type":"stream","text":["Captioning:   0%|          | 0/200 [01:34\u003c?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4119516349.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Use batch_size=1 for stability, increase for speed (may cause OOM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 18\u001b[0;31m results = run_inference(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmanifest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_manifest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1968464426.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(manifest, dataset, prompts, batch_size, max_frames, fps)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 309\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1968464426.py\u001b[0m in \u001b[0;36mcaption_single\u001b[0;34m(video_path, prompt, max_frames, fps)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 191\u001b[0;31m         \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     generated_ids_trimmed = [\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         )\n\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1476\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1477\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **kwargs)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_ids\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1305\u001b[0;31m         outputs = self.language_model(\n\u001b[0m\u001b[1;32m   1306\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 891\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 741\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    742\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 680\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Define prompts --\u003e put your all your prompt here\n","PROMPTS = {\n","    \"curriculum_icl\": CURRICULUM_ICL_PROMPT,          # Stephen ICL 1\n","    \"contrastive_icl\": CONTRASTIVE_ICL_PROMPT,        # Stephen ICL 2\n","    \"role_based_icl\": ROLE_BASED_ICL_PROMPT,          # Stephen ICL 3\n","}\n","\n","\n","# Optional: Select random subset (set to None for all videos)\n","NUM_SAMPLES = 200\n","RANDOM_SEED = 42\n","\n","# Select samples\n","selected_manifest = select_samples(manifest, n=NUM_SAMPLES, seed=RANDOM_SEED)\n","\n","# Run inference\n","# Use batch_size=1 for stability, increase for speed (may cause OOM)\n","results = run_inference(\n","    manifest=selected_manifest,\n","    dataset=ds,\n","    prompts=PROMPTS,\n","    batch_size=CONFIG[\"batch_size\"],\n","    max_frames=CONFIG[\"max_frames\"],\n","    fps=CONFIG[\"fps\"],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39578,"status":"ok","timestamp":1764314766077,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"xFKC1LSo2YLB","outputId":"e319ae49-a488-4c2b-d0bc-4875f7ee3679"},"outputs":[{"name":"stderr","output_type":"stream","text":["Captioning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:39\u003c00:00,  7.91s/it]"]},{"name":"stdout","output_type":"stream","text":["‚úì Completed: 5 videos\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["failed_global = [780, 364, 851, 741, 617]\n","\n","failed_manifest = [m for m in selected_manifest if m[\"idx\"] in failed_global]\n","\n","len(failed_manifest), failed_manifest\n","\n","recovered_results = run_inference(\n","    manifest=failed_manifest,\n","    dataset=ds,\n","    prompts=PROMPTS,\n","    batch_size=1,\n","    max_frames=14,\n","    fps=CONFIG[\"fps\"],\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1764314778418,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"OvAYvYnT5ajE","outputId":"ae7dec8b-ce5e-4ea5-d7fa-90a17146d154"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final count: 200\n"]}],"source":["# Build lookup for recovered rows\n","fix_map = {r[\"row_idx\"]: r for r in recovered_results}\n","\n","# Update existing rows\n","results = [fix_map.get(r[\"row_idx\"], r) for r in results]\n","\n","# Add missing recovered rows that never appeared in results\n","existing_ids = {r[\"row_idx\"] for r in results}\n","\n","missing_to_add = [\n","    r for r in recovered_results\n","    if r[\"row_idx\"] not in existing_ids\n","]\n","\n","# Append them\n","results.extend(missing_to_add)\n","\n","print(\"Final count:\", len(results))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1764307176154,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"eXbp1jKc5nOY","outputId":"dfef7d80-ce20-4bb0-ed1a-bd83ad24c803"},"outputs":[{"data":{"text/plain":["200"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(results)"]},{"cell_type":"markdown","metadata":{"id":"ziKBhEvexZL5"},"source":["# Export Result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"executionInfo":{"elapsed":147,"status":"ok","timestamp":1764314792569,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"b9TWP_M0xHha","outputId":"843318c4-8fef-45e2-f04a-24c0add9c711"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Saved to: /content/drive/MyDrive/CameraBench/outputs/qwen25_ft_captions_200samples_20251128_072632.xlsx\n","üìä Total rows: 200\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"row_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 300,\n        \"min\": 3,\n        \"max\": 1054,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          970,\n          96,\n          274\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos_gif/ti231UvSvfQ.12.2.gif\",\n          \"https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos_gif/1hu1np_BI2M.11.4.gif\",\n          \"https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos_gif/7xltSb3VBJQ.2.4.gif\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"human_caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 166,\n        \"samples\": [\n          \"The camera smoothly and steadily moves upward, exhibiting only minor movement.\",\n          \"The first-person camera moves forward along the street with noticeable shaky movement, then slightly pans to the right, maintaining an unsteady and shaky motion throughout.\",\n          \"The camera remains mostly fixed, exhibiting only minor movement without any clear or intentional direction, while maintaining a very smooth and steady presence throughout.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"curriculum_icl\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"CAMERA_MOTION: The camera remains fixed but slightly unsteady, with no intentional movement.\\nSCENE_DESCRIPTION: A phone and some stationery items rest on a table, accompanied by a partially visible picture frame.\",\n          \"CAMERA_MOTION: The camera smoothly descends while simultaneously tilting upward, maintaining a steady and fluid motion throughout.\\nSCENE_DESCRIPTION: A computer workstation with dual monitors, one displaying a file directory and the other showing a text editor, accompanied by various office supplies and a leather chair.\",\n          \"CAMERA_MOTION: The handheld camera smoothly pans left to adjust its facing while continuously dollying forward to track the street view, maintaining minimal shaking throughout the movement.\\nSCENE_DESCRIPTION: Residential buildings line both sides of the street, with some houses featuring visible front yards and a few cars parked along the road.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contrastive_icl\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 196,\n        \"samples\": [\n          \"CAMERA_MOTION: The camera arcs clockwise with a slightly unsteady motion, exhibiting some shaking as it moves.\\nSCENE_DESCRIPTION: A black Mercedes car with gold interior doors is parked, and the camera smoothly moves clockwise around it.\",\n          \"CAMERA_MOTION: The camera smoothly dollies backward while simultaneously trucking right, maintaining minimal shaking throughout the movement.\\nSCENE_DESCRIPTION: A person rides a motorcycle backward, kicking up sand, with a car following close behind.\",\n          \"CAMERA_MOTION: The camera smoothly moves forward, maintaining a steady and fluid motion throughout.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"role_based_icl\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"CAMERA_MOTION: The camera remains fixed but slightly unsteady, with no intentional movement.\\nSCENE_DESCRIPTION: A phone and some stationery are on a table, partially obscuring a picture of a dolphin jumping out of the water.\",\n          \"CAMERA_MOTION: The camera smoothly descends while simultaneously tilting upward, maintaining a steady and fluid motion throughout.\\nSCENE_DESCRIPTION: A computer workstation with dual monitors, a keyboard, a mouse, and various scattered items such as CDs, papers, and a chair.\",\n          \"CAMERA_MOTION: The handheld camera smoothly moves forward, panning to the left to reveal the other side of the street, maintaining minimal shaking throughout.\\nSCENE_DESCRIPTION: Residential houses line both sides of the street, with no people currently visible.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df"},"text/html":["\n","  \u003cdiv id=\"df-ab47a864-2cbb-4eea-b55a-a2a4b184f4c5\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003erow_idx\u003c/th\u003e\n","      \u003cth\u003evideo_url\u003c/th\u003e\n","      \u003cth\u003elabels\u003c/th\u003e\n","      \u003cth\u003ehuman_caption\u003c/th\u003e\n","      \u003cth\u003ecurriculum_icl\u003c/th\u003e\n","      \u003cth\u003econtrastive_icl\u003c/th\u003e\n","      \u003cth\u003erole_based_icl\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e297\u003c/td\u003e\n","      \u003ctd\u003ehttps://huggingface.co/datasets/syCen/CameraBe...\u003c/td\u003e\n","      \u003ctd\u003e[no-shaking, complex-motion, regular-speed, pe...\u003c/td\u003e\n","      \u003ctd\u003eThe camera ascends smoothly while tilting down...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera smoothly descends wh...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera smoothly descends wh...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera smoothly descends wh...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e130\u003c/td\u003e\n","      \u003ctd\u003ehttps://huggingface.co/datasets/syCen/CameraBe...\u003c/td\u003e\n","      \u003ctd\u003e[minimal-shaking, complex-motion, regular-spee...\u003c/td\u003e\n","      \u003ctd\u003eThe camera tilts upward smoothly with minimal ...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera tilts upward smoothl...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera tilts upward smoothl...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera tilts upward smoothl...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e626\u003c/td\u003e\n","      \u003ctd\u003ehttps://huggingface.co/datasets/syCen/CameraBe...\u003c/td\u003e\n","      \u003ctd\u003e[minimal-shaking, no-motion, regular-speed]\u003c/td\u003e\n","      \u003ctd\u003eThe camera remains fixed but slightly unsteady...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera arcs slowly and smoo...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera remains fixed but sl...\u003c/td\u003e\n","      \u003ctd\u003eCAMERA_MOTION: The camera remains fixed but sl...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab47a864-2cbb-4eea-b55a-a2a4b184f4c5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-ab47a864-2cbb-4eea-b55a-a2a4b184f4c5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ab47a864-2cbb-4eea-b55a-a2a4b184f4c5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","    \u003cdiv id=\"df-553f3ebf-cbe0-4d9a-86e6-db46e07b0448\"\u003e\n","      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-553f3ebf-cbe0-4d9a-86e6-db46e07b0448')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","      \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() =\u003e {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-553f3ebf-cbe0-4d9a-86e6-db46e07b0448 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["   row_idx                                          video_url  \\\n","0      297  https://huggingface.co/datasets/syCen/CameraBe...   \n","1      130  https://huggingface.co/datasets/syCen/CameraBe...   \n","2      626  https://huggingface.co/datasets/syCen/CameraBe...   \n","\n","                                              labels  \\\n","0  [no-shaking, complex-motion, regular-speed, pe...   \n","1  [minimal-shaking, complex-motion, regular-spee...   \n","2        [minimal-shaking, no-motion, regular-speed]   \n","\n","                                       human_caption  \\\n","0  The camera ascends smoothly while tilting down...   \n","1  The camera tilts upward smoothly with minimal ...   \n","2  The camera remains fixed but slightly unsteady...   \n","\n","                                      curriculum_icl  \\\n","0  CAMERA_MOTION: The camera smoothly descends wh...   \n","1  CAMERA_MOTION: The camera tilts upward smoothl...   \n","2  CAMERA_MOTION: The camera arcs slowly and smoo...   \n","\n","                                     contrastive_icl  \\\n","0  CAMERA_MOTION: The camera smoothly descends wh...   \n","1  CAMERA_MOTION: The camera tilts upward smoothl...   \n","2  CAMERA_MOTION: The camera remains fixed but sl...   \n","\n","                                      role_based_icl  \n","0  CAMERA_MOTION: The camera smoothly descends wh...  \n","1  CAMERA_MOTION: The camera tilts upward smoothl...  \n","2  CAMERA_MOTION: The camera remains fixed but sl...  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from datetime import datetime\n","\n","# Create DataFrame\n","df = pd.DataFrame(results)\n","\n","# Generate filename with timestamp\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","n_samples = len(results)\n","excel_filename = f\"qwen25_redone_captions_{n_samples}samples_{timestamp}.xlsx\"\n","EXCEL_PATH = os.path.join(PATHS[\"outputs\"], excel_filename)\n","\n","# Export to Excel\n","with pd.ExcelWriter(EXCEL_PATH, engine=\"xlsxwriter\") as writer:\n","    df.to_excel(writer, index=False, sheet_name=\"captions\")\n","\n","    # Auto-adjust column widths\n","    worksheet = writer.sheets[\"captions\"]\n","    for i, col in enumerate(df.columns):\n","        max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2\n","        worksheet.set_column(i, i, min(max_len, 60))\n","\n","print(f\"‚úÖ Saved to: {EXCEL_PATH}\")\n","print(f\"üìä Total rows: {len(df)}\")\n","df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKIZz3elxHQb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3U29jxGuj-Rj"},"source":["### Sanity Check"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"elapsed":74,"status":"error","timestamp":1764536354626,"user":{"displayName":"Stephen Dong","userId":"04348147209048929483"},"user_tz":300},"id":"Dx4avhjhj--W","outputId":"764bda6a-480b-4500-9939-86a44ee760ff"},"outputs":[{"ename":"NameError","evalue":"name 'Qwen2_5_VLForConditionalGeneration' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3451574589.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m model_base = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"Qwen/Qwen2.5-VL-7B-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Qwen2_5_VLForConditionalGeneration' is not defined"]}],"source":["# Load pre-trained base model\n","model_base = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n","    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n","    torch_dtype=\"auto\",\n","    device_map=\"auto\"\n",")\n","processor_base = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n","\n","# Load fine-tuned CameraBench model\n","model_ft = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n","    \"chancharikm/qwen2.5-vl-7b-cam-motion\",\n","    torch_dtype=\"auto\",\n","    device_map=\"auto\"\n",")\n","processor_ft = AutoProcessor.from_pretrained(\"chancharikm/qwen2.5-vl-7b-cam-motion\")\n","\n","print(\"Loaded both base + FT models ‚úì\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","name":"","provenance":[{"file_id":"1BSD3bjjdCCv2R5wjCbQFSLXq2m-oL9Ta","timestamp":1764227080026}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"18a18fc0bfac4378be8bb56b1cad6685":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85715b6c375e45e0891de8812d8ae860","IPY_MODEL_994af33ff878484189897f02b53ce48f","IPY_MODEL_a53bc3c2e8db46b9beb1eae7ff0266c8"],"layout":"IPY_MODEL_c933d3c2b5954156b53e5028b342bb14"}},"27e03d904c1347cbb6ff97abad4cae44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d567d9a1e024010876f7b991157693c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3720f97ad05d4d298a88ca4a3386bb46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"748d67a528f64250946306d9c7d74dd8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85715b6c375e45e0891de8812d8ae860":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3720f97ad05d4d298a88ca4a3386bb46","placeholder":"‚Äã","style":"IPY_MODEL_baaf8a98ec614167905d28703c74654a","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"994af33ff878484189897f02b53ce48f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_27e03d904c1347cbb6ff97abad4cae44","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff0ae2ed40a34776a0cad6b8d14bec25","value":5}},"a53bc3c2e8db46b9beb1eae7ff0266c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_748d67a528f64250946306d9c7d74dd8","placeholder":"‚Äã","style":"IPY_MODEL_2d567d9a1e024010876f7b991157693c","value":"‚Äá5/5‚Äá[00:00\u0026lt;00:00,‚Äá37.85it/s]"}},"baaf8a98ec614167905d28703c74654a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c933d3c2b5954156b53e5028b342bb14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff0ae2ed40a34776a0cad6b8d14bec25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}