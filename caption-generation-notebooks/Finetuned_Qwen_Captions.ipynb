{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33539,"status":"ok","timestamp":1762455215599,"user":{"displayName":"Audrey Tjokro","userId":"14164874831900406086"},"user_tz":300},"id":"DsmQyrjMwtT0","outputId":"6d979dfd-c434-49b6-b3f9-321a183a66de"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hTorch: 2.9.0+cu128\n","CUDA available: True\n","GPU: Tesla T4\n","Torchvision: 0.24.0+cu128\n","âœ… All dependencies installed correctly\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  _C._set_float32_matmul_precision(precision)\n"]}],"source":["# Cell 1: Setup & Install - COMPLETE WORKING VERSION\n","\n","# Install all packages WITH torchvision (it's required!)\n","!pip install -q --upgrade torch torchvision transformers datasets imageio imageio-ffmpeg decord tqdm bitsandbytes accelerate\n","!pip install -q \"qwen-vl-utils[decord]==0.0.8\"\n","\n","import torch\n","print(\"Torch:\", torch.__version__)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","\n","# Verify torchvision is installed\n","try:\n","    import torchvision\n","    print(\"Torchvision:\", torchvision.__version__)\n","    print(\"âœ… All dependencies installed correctly\")\n","except ImportError:\n","    print(\"âŒ Torchvision missing - this shouldn't happen!\")\n","\n","torch.set_float32_matmul_precision(\"high\")"]},{"cell_type":"markdown","metadata":{"id":"HnxB3j3pfmlq"},"source":["cell 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1762455215615,"user":{"displayName":"Audrey Tjokro","userId":"14164874831900406086"},"user_tz":300},"id":"pUgP8I4mxODR","outputId":"b73e4fef-8568-4f96-f79a-041af50a13c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Root: /content/camerabench\n","Videos: /content/camerabench/videos_gif_mp4\n","Outputs: /content/camerabench/outputs\n"]}],"source":["# --- Colab: Create folders ---\n","import os, json\n","\n","ROOT = \"/content/camerabench\"\n","VIDS = f\"{ROOT}/videos_gif_mp4\"\n","OUTS = f\"{ROOT}/outputs\"\n","\n","for p in (ROOT, VIDS, OUTS):\n","    os.makedirs(p, exist_ok=True)\n","\n","print(\"Root:\", ROOT)\n","print(\"Videos:\", VIDS)\n","print(\"Outputs:\", OUTS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXGcEtPZxPp_"},"outputs":[],"source":["# --- PATCH: re-define GIFâ†’MP4 utils with even-dimension padding ---\n","import imageio, imageio.v3 as iio\n","import numpy as np, os, warnings\n","from typing import Tuple, List\n","from PIL import Image\n","from decord import VideoReader, cpu\n","\n","def to_rgb_frame(arr: np.ndarray) -> np.ndarray:\n","    # PIL-safe conversion path\n","    if isinstance(arr, Image.Image):\n","        if arr.mode in (\"RGBA\", \"LA\"):\n","            arr = arr.convert(\"RGBA\")\n","            bg = Image.new(\"RGBA\", arr.size, (0, 0, 0, 0))\n","            bg.alpha_composite(arr)\n","            arr = bg.convert(\"RGB\")\n","        else:\n","            arr = arr.convert(\"RGB\")\n","        return np.array(arr, dtype=np.uint8)\n","\n","    # NumPy path\n","    if arr.dtype != np.uint8:\n","        a = arr.astype(np.float32)\n","        a = np.clip(a, 0, 255)\n","        arr = a.astype(np.uint8)\n","\n","    if arr.ndim == 2:  # gray -> RGB\n","        arr = np.stack([arr, arr, arr], axis=-1)\n","\n","    if arr.shape[-1] == 4:  # RGBA -> RGB (alpha over black)\n","        rgb = arr[..., :3].astype(np.float32)\n","        alpha = (arr[..., 3:4].astype(np.float32) / 255.0)\n","        rgb = (rgb * alpha).astype(np.uint8)\n","        arr = rgb\n","\n","    if arr.shape[-1] != 3:\n","        first = arr[..., 0]\n","        arr = np.stack([first, first, first], axis=-1).astype(np.uint8)\n","\n","    return arr\n","\n","def _ensure_same_size(frames: List[np.ndarray]) -> Tuple[List[np.ndarray], Tuple[int, int]]:\n","    \"\"\"Resize all frames to the first frame's WxH (consistent encoder input).\"\"\"\n","    if not frames:\n","        return frames, (0, 0)\n","    h, w = frames[0].shape[:2]\n","    out = []\n","    for f in frames:\n","        if f.shape[0] != h or f.shape[1] != w:\n","            pil = Image.fromarray(f)\n","            pil = pil.resize((w, h), resample=Image.Resampling.BILINEAR)\n","            f = np.array(pil, dtype=np.uint8)\n","        out.append(f)\n","    return out, (h, w)\n","\n","def _pad_to_even(frames: List[np.ndarray]) -> Tuple[List[np.ndarray], Tuple[int, int]]:\n","    \"\"\"Pad frames on the right/bottom by 1 pixel if width or height is odd (needed for yuv420p).\"\"\"\n","    if not frames:\n","        return frames, (0, 0)\n","    h, w = frames[0].shape[:2]\n","    pad_h = h % 2\n","    pad_w = w % 2\n","    if pad_h == 0 and pad_w == 0:\n","        return frames, (h, w)\n","\n","    H, W = h + pad_h, w + pad_w\n","    out = []\n","    for f in frames:\n","        canvas = np.zeros((H, W, 3), dtype=np.uint8)\n","        canvas[:h, :w, :] = f\n","        out.append(canvas)\n","    return out, (H, W)\n","\n","def decord_ok(path: str) -> bool:\n","    try:\n","        if not (os.path.exists(path) and os.path.getsize(path) > 0):\n","            return False\n","        vr = VideoReader(path, ctx=cpu(0))\n","        if len(vr) < 1:\n","            return False\n","        _ = vr[0]\n","        return True\n","    except Exception:\n","        return False\n","\n","def convert_gif_to_mp4(gif_path: str, mp4_path: str, fps: float = 8.0) -> bool:\n","    \"\"\"\n","    Robust GIFâ†’MP4:\n","      - normalize to RGB\n","      - force consistent WxH\n","      - **pad to even dims** for yuv420p\n","      - macro_block_size=1 to avoid implicit resizing\n","    \"\"\"\n","    try:\n","        frames = []\n","        with imageio.get_reader(gif_path) as reader:\n","            for frame in reader:\n","                frames.append(to_rgb_frame(frame))\n","        if not frames:\n","            warnings.warn(f\"No frames decoded from GIF: {gif_path}\")\n","            return False\n","\n","        frames, _ = _ensure_same_size(frames)\n","        frames, _ = _pad_to_even(frames)  # <-- critical fix for yuv420p\n","\n","        imageio.mimsave(\n","            mp4_path,\n","            frames,\n","            fps=fps,\n","            macro_block_size=1,\n","            codec=\"libx264\",\n","            format=\"FFMPEG\",\n","            ffmpeg_params=[\"-pix_fmt\", \"yuv420p\", \"-movflags\", \"+faststart\"],\n","        )\n","        return decord_ok(mp4_path)\n","    except Exception as e:\n","        warnings.warn(\n","            f\"GIFâ†’MP4 failed for {gif_path} -> {mp4_path}: {e}\\n\\n\"\n","            \"Tip: this is often due to odd frame sizes with yuv420p; padding to even dims usually fixes it.\"\n","        )\n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1790431,"status":"ok","timestamp":1762452414514,"user":{"displayName":"Audrey Tjokro","userId":"14164874831900406086"},"user_tz":300},"id":"PQSc5v-SxQ_n","outputId":"59dad10c-660d-44af-a9e1-2585dff177a6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Building GIFâ†’MP4 manifest:   8%|â–Š         | 82/1071 [00:01<00:33, 29.87it/s]/tmp/ipython-input-3367234834.py:31: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n","  alpha = (arr[..., 3:4].astype(np.float32) / 255.0)\n","/tmp/ipython-input-3367234834.py:32: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n","  rgb = (rgb * alpha).astype(np.uint8)\n","Building GIFâ†’MP4 manifest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1071/1071 [29:49<00:00,  1.67s/it]"]},{"name":"stdout","output_type":"stream","text":["Saved manifest: /content/camerabench/outputs/gif2mp4_manifest.json\n","Total items: 1071\n","Readable MP4s: 1063\n","\n","Sample manifest entry:\n","{\n","  \"row_idx\": 0,\n","  \"rel_path\": \"videos/-2uIa-XMJC0.5.3.mp4\",\n","  \"gif_url\": \"https://huggingface.co/datasets/syCen/CameraBench/resolve/main/videos_gif/-2uIa-XMJC0.5.3.gif\",\n","  \"local_gif\": \"/content/camerabench/videos_gif_mp4/3e1afda20f104270_-2uIa-XMJC0.5.3.gif\",\n","  \"local_mp4\": \"/content/camerabench/videos_gif_mp4/3e1afda20f104270_-2uIa-XMJC0.5.3.mp4\",\n","  \"status\": \"ok\"\n","}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# --- Colab: Load dataset & build manifest (download GIFs, convert to MP4) ---\n","import os, json, time, hashlib, requests\n","from datasets import load_dataset\n","from tqdm import tqdm\n","\n","MANIFEST_PATH = f\"{OUTS}/gif2mp4_manifest.json\"\n","\n","# Load split=\"test\"\n","ds = load_dataset(\"syCen/CameraBench\", split=\"test\")\n","\n","def safe_filename(url: str) -> str:\n","    \"\"\"Make a deterministic, safe base name from URL.\"\"\"\n","    h = hashlib.md5(url.encode(\"utf-8\")).hexdigest()[:16]\n","    base = os.path.basename(url).split(\"?\")[0]\n","    return f\"{h}_{base}\"\n","\n","def download_gif(url: str, out_path: str, timeout=20, retries=3) -> bool:\n","    for i in range(retries):\n","        try:\n","            r = requests.get(url, stream=True, timeout=timeout)\n","            if r.status_code == 200:\n","                with open(out_path, \"wb\") as f:\n","                    for chunk in r.iter_content(chunk_size=1024 * 256):\n","                        if chunk:\n","                            f.write(chunk)\n","                return os.path.exists(out_path) and os.path.getsize(out_path) > 0\n","        except Exception:\n","            time.sleep(1.0 * (i + 1))\n","    return False\n","\n","manifest = []\n","for idx, row in enumerate(tqdm(ds, desc=\"Building GIFâ†’MP4 manifest\")):\n","    gif_url = row.get(\"video\", None) or row.get(\"Video\", None)\n","    rel_path = row.get(\"path\", None)  # relative mp4 path/id in dataset metadata\n","    if not gif_url:\n","        continue\n","\n","    gif_name = safe_filename(gif_url)\n","    local_gif = os.path.join(VIDS, gif_name if gif_name.lower().endswith(\".gif\") else gif_name + \".gif\")\n","    local_mp4 = os.path.splitext(local_gif)[0] + \".mp4\"\n","\n","    # Download if not present\n","    if not os.path.exists(local_gif):\n","        ok = download_gif(gif_url, local_gif)\n","        if not ok:\n","            # Skip adding broken downloads; entry records failure state\n","            manifest.append({\n","                \"row_idx\": idx, \"rel_path\": rel_path, \"gif_url\": gif_url,\n","                \"local_gif\": local_gif, \"local_mp4\": local_mp4, \"status\": \"gif_download_failed\"\n","            })\n","            continue\n","\n","    # Convert to MP4 if needed\n","    if not os.path.exists(local_mp4):\n","        _ = convert_gif_to_mp4(local_gif, local_mp4, fps=8.0)\n","\n","    status = \"ok\" if decord_ok(local_mp4) else \"mp4_unreadable\"\n","    manifest.append({\n","        \"row_idx\": idx,\n","        \"rel_path\": rel_path,\n","        \"gif_url\": gif_url,\n","        \"local_gif\": local_gif,\n","        \"local_mp4\": local_mp4,\n","        \"status\": status,\n","    })\n","\n","# Save manifest\n","with open(MANIFEST_PATH, \"w\") as f:\n","    json.dump(manifest, f, indent=2)\n","\n","print(f\"Saved manifest: {MANIFEST_PATH}\")\n","print(\"Total items:\", len(manifest))\n","print(\"Readable MP4s:\", sum(1 for m in manifest if m[\"status\"] == \"ok\"))\n","print(\"\\nSample manifest entry:\")\n","print(json.dumps(next((m for m in manifest if m[\"status\"] == \"ok\"), manifest[0] if manifest else {}), indent=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254,"referenced_widgets":["68f7d203c4794c09934326ba896f45a0","334e816a82d14d52a64feff75c34a44c","8d174f513d534eeeb0b32c33701362e5","7e3dea4f9698445d8c63a5e103215ae3","82602070723845519c932356fbeb2686","750c9b254e494841a9bc4529b13b4530","e9144e12cc2e4af8b44f7abfaa0c712c","408b17f1477048d08610d2831501e23d","b7fda273f728465cb3307a5124c38642","7570cc0fba5b4fa4baa7456479c89d62","e354ea05d8a246299375c389a8694acb","b269698d81654592b43ee4d29d1d66b9","9f01b188f1d94023b89caf8dbe089c3f","b730aebeb7b44b1787a71fff81c417f1","6b98798fa56644f4830c73f0fa1c2729","3b0d704ca11e4cfa9d9f7ca7b51df012","184e8eeabdf74761848572bd874f03c5","1de866ce5334410195dbaac349c7c599","e8aed84793fa4fe79d566798956ecba3","9f165c083442421d8efa004fa009694f","9921c108f6c84c9c9836c0dff81ed1b4","6b7c189167b042359b3366570ec147f8"]},"executionInfo":{"elapsed":80925,"status":"ok","timestamp":1762457258143,"user":{"displayName":"Audrey Tjokro","userId":"14164874831900406086"},"user_tz":300},"id":"cUCqkNhhxSjw","outputId":"dd012923-d96d-4a31-e793-ad9bb586afae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading chancharikm/qwen2.5-vl-7b-cam-motion...\n","Loading processor...\n","âœ… Processor loaded\n","Loading model with 4-bit quantization...\n","Using Qwen2_5_VLForConditionalGeneration...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68f7d203c4794c09934326ba896f45a0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b269698d81654592b43ee4d29d1d66b9","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["âœ… Loaded with Qwen2.5-VL class\n","âœ… Model loaded on cuda:0\n","Model type: Qwen2_5_VLForConditionalGeneration\n","Has generate: True\n","ðŸŽ‰ Ready to generate captions!\n"]}],"source":["# Cell 5: Load Model & Processor - ROBUST VERSION\n","\n","import torch\n","from transformers import BitsAndBytesConfig, AutoProcessor\n","\n","MODEL_ID = \"chancharikm/qwen2.5-vl-7b-cam-motion\"\n","\n","# Configure 4-bit quantization\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_quant=False,\n","    bnb_4bit_quant_type=\"nf4\",\n",")\n","\n","print(f\"Loading {MODEL_ID}...\")\n","\n","# Load processor\n","print(\"Loading processor...\")\n","processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n","print(\"âœ… Processor loaded\")\n","\n","# Try multiple model loading approaches\n","print(\"Loading model with 4-bit quantization...\")\n","\n","# Approach 1: Try Qwen2.5-VL specific class\n","try:\n","    from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration\n","    print(\"Using Qwen2_5_VLForConditionalGeneration...\")\n","    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n","        MODEL_ID,\n","        device_map=\"auto\",\n","        quantization_config=bnb_config,\n","        trust_remote_code=True,\n","        torch_dtype=torch.bfloat16,\n","        attn_implementation=\"sdpa\",\n","    )\n","    print(\"âœ… Loaded with Qwen2.5-VL class\")\n","\n","except (ImportError, AttributeError) as e:\n","    print(f\"Approach 1 failed: {e}\")\n","\n","    # Approach 2: Use AutoModelForVision2Seq\n","    try:\n","        from transformers import AutoModelForVision2Seq\n","        print(\"Using AutoModelForVision2Seq...\")\n","        model = AutoModelForVision2Seq.from_pretrained(\n","            MODEL_ID,\n","            device_map=\"auto\",\n","            quantization_config=bnb_config,\n","            trust_remote_code=True,\n","            torch_dtype=torch.bfloat16,\n","            attn_implementation=\"sdpa\",\n","        )\n","        print(\"âœ… Loaded with AutoModelForVision2Seq\")\n","\n","    except Exception as e2:\n","        print(f\"Approach 2 failed: {e2}\")\n","\n","        # Approach 3: Load with trust_remote_code (uses model's own code)\n","        print(\"Using trust_remote_code to load custom model code...\")\n","        from transformers import AutoModelForCausalLM\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            device_map=\"auto\",\n","            quantization_config=bnb_config,\n","            trust_remote_code=True,\n","            torch_dtype=torch.bfloat16,\n","            attn_implementation=\"sdpa\",\n","        )\n","        print(\"âœ… Loaded with trust_remote_code\")\n","\n","# Verify model has generate method\n","if not hasattr(model, 'generate'):\n","    raise RuntimeError(f\"Model type {type(model).__name__} does not have generate() method!\")\n","\n","model.eval()\n","for p in model.parameters():\n","    p.requires_grad_(False)\n","\n","print(f\"âœ… Model loaded on {model.device}\")\n","print(f\"Model type: {type(model).__name__}\")\n","print(f\"Has generate: {hasattr(model, 'generate')}\")\n","print(\"ðŸŽ‰ Ready to generate captions!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138537,"status":"ok","timestamp":1762457619826,"user":{"displayName":"Audrey Tjokro","userId":"14164874831900406086"},"user_tz":300},"id":"XEurAn46xUNz","outputId":"c40ffeba-e3d5-4234-de55-5384f1551d4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing on 5 videos\n","\n","Video: 3e1afda20f104270_-2uIa-XMJC0.5.3.mp4\n","Time: 1.54s prep + 30.36s gen = 31.90s total\n","Caption: The camera smoothly tracks a person skateboarding on a glass platform, initially positioned in the upper right. As they move left, the camera pans left to keep them centered, maintaining a steady and fluid motion throughout.\n","\n","Video: d9faf33e350a923c_0TqQje61Hoo.3.1.mp4\n","Time: 0.44s prep + 22.37s gen = 22.82s total\n","Caption: The camera arcs counterclockwise with a smooth, steady motion, exhibiting minimal shaking.\n","\n","Video: 0ab6b40c0b725ae6_0BmANHSUbJg.3.5.mp4\n","Time: 0.54s prep + 25.80s gen = 26.34s total\n","Caption: The camera moves slowly backward with a slight upward motion, maintaining a very smooth and steady trajectory without any shaking.\n","\n","Video: 693c83b4ba7607f7_1018.1.7.mp4\n","Time: 0.59s prep + 26.76s gen = 27.35s total\n","Caption: The camera smoothly moves backward, leading the subject from the front with minimal shaking.\n","\n","Video: 7c7ee1bea4808828_2254244.3.mp4\n","Time: 0.59s prep + 29.37s gen = 29.96s total\n","Caption: The camera arcs clockwise around the building, transitioning smoothly from the side to the top, and slightly zooms in to emphasize the subject, all with very smooth movement and no shaking.\n","\n","âœ… Sanity test complete!\n"]}],"source":["# Cell 6: Sanity test - FIXED for Qwen2.5-VL\n","\n","import time\n","import json\n","from qwen_vl_utils import process_vision_info\n","\n","ROOT = \"/content/camerabench\"\n","OUTS = f\"{ROOT}/outputs\"\n","MANIFEST_PATH = f\"{OUTS}/gif2mp4_manifest.json\"\n","\n","PROMPT_TXT = \"Describe the camera motions.\"\n","\n","# Load manifest\n","with open(MANIFEST_PATH, \"r\") as f:\n","    manifest = json.load(f)\n","\n","test_items = [m for m in manifest if m[\"status\"] == \"ok\"][:5]\n","print(f\"Testing on {len(test_items)} videos\\n\")\n","\n","def generate_caption_for_path(video_path: str):\n","    # Build messages\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"video\", \"video\": video_path, \"fps\": 6.0, \"max_frames\": 32},\n","                {\"type\": \"text\", \"text\": PROMPT_TXT},\n","            ],\n","        }\n","    ]\n","\n","    t0 = time.time()\n","\n","    # Apply processor directly to messages (this is the key fix!)\n","    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    image_inputs, video_inputs = process_vision_info(messages)\n","\n","    inputs = processor(\n","        text=[text],  # Use the templated text\n","        images=image_inputs,\n","        videos=video_inputs,\n","        padding=True,\n","        return_tensors=\"pt\",\n","    )\n","\n","    t_prep = time.time() - t0\n","\n","    # Move to device\n","    inputs = inputs.to(model.device)\n","\n","    t1 = time.time()\n","    generated_ids = model.generate(\n","        **inputs,\n","        max_new_tokens=128,  # Increased for better captions\n","    )\n","    t_gen = time.time() - t1\n","\n","    # Trim the input tokens from output\n","    generated_ids_trimmed = [\n","        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n","    ]\n","\n","    output_text = processor.batch_decode(\n","        generated_ids_trimmed,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )[0]\n","\n","    return output_text.strip(), t_prep, t_gen\n","\n","# Test on videos\n","for it in test_items:\n","    vp = it[\"local_mp4\"]\n","    try:\n","        cap, tprep, tgen = generate_caption_for_path(vp)\n","        print(f\"Video: {os.path.basename(vp)}\")\n","        print(f\"Time: {tprep:.2f}s prep + {tgen:.2f}s gen = {tprep+tgen:.2f}s total\")\n","        print(f\"Caption: {cap}\\n\")\n","    except Exception as e:\n","        print(f\"Video: {os.path.basename(vp)}\")\n","        print(f\"âŒ Error: {e}\\n\")\n","        import traceback\n","        traceback.print_exc()\n","\n","print(\"âœ… Sanity test complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"xOyLIy5MxVy6","outputId":"0b4360be-b34f-4317-d2c8-b06d12ee66dc"},"outputs":[{"ename":"ValueError","evalue":"mount failed","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1349081644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/camerabench\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["# Cell 7: ROBUST chunked captioning - Handles memory & long videos\n","\n","from google.colab import drive\n","from qwen_vl_utils import process_vision_info\n","from tqdm import tqdm\n","from datasets import load_dataset\n","import gc\n","import json\n","import os\n","import torch\n","\n","drive.mount('/content/drive')\n","\n","ROOT = \"/content/camerabench\"\n","OUTS = f\"{ROOT}/outputs\"\n","MANIFEST_PATH = f\"{OUTS}/gif2mp4_manifest.json\"\n","DRIVE_FOLDER = \"/content/drive/MyDrive/Deep Learning Fall 2025\"\n","JSONL_PATH = f\"{DRIVE_FOLDER}/motion_captions_qwen_gif.jsonl\"\n","\n","os.makedirs(DRIVE_FOLDER, exist_ok=True)\n","\n","VIDEOS_PER_CHUNK = 100\n","\n","print(\"Loading manifest and dataset...\")\n","with open(MANIFEST_PATH, \"r\") as f:\n","    manifest = json.load(f)\n","\n","ds = load_dataset(\"syCen/CameraBench\", split=\"test\")\n","\n","done = set()\n","if os.path.exists(JSONL_PATH):\n","    print(f\"Found existing progress file in Google Drive!\")\n","    with open(JSONL_PATH, \"r\") as f:\n","        for line in f:\n","            line = line.strip()\n","            if not line:\n","                continue\n","            try:\n","                obj = json.loads(line)\n","                if \"row_idx\" in obj:\n","                    done.add(int(obj[\"row_idx\"]))\n","            except:\n","                pass\n","\n","processable = [m for m in manifest if m.get(\"status\") == \"ok\" and int(m[\"row_idx\"]) not in done]\n","\n","total_done = len(done)\n","total_remaining = len(processable)\n","total_videos = len([m for m in manifest if m.get(\"status\") == \"ok\"])\n","\n","print(f\"\\n{'='*70}\")\n","print(f\"CAPTIONING STATUS (Google Drive)\")\n","print(f\"{'='*70}\")\n","print(f\"âœ… Already completed: {total_done} videos\")\n","print(f\"ðŸ“ Remaining: {total_remaining} videos\")\n","print(f\"ðŸŽ¯ Total readable videos: {total_videos}\")\n","print(f\"\\nâ–¶ï¸  This run will process: {min(VIDEOS_PER_CHUNK, total_remaining)} videos\")\n","print(f\"ðŸ’¾ Progress file: {JSONL_PATH}\")\n","print(f\"{'='*70}\\n\")\n","\n","if total_remaining == 0:\n","    print(\"ðŸŽ‰ All videos already captioned! Run Cell 9 to export to Excel.\")\n","else:\n","    chunk_to_process = processable[:VIDEOS_PER_CHUNK]\n","\n","    ok_cnt, skip_cnt = 0, 0\n","\n","    def caption_video(vpath: str, max_frames: int = 32) -> str:\n","        \"\"\"Generate caption with fallback for problematic videos.\"\"\"\n","        messages = [{\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"video\", \"video\": vpath, \"fps\": 6.0, \"max_frames\": max_frames},\n","                {\"type\": \"text\", \"text\": \"Describe the camera motions.\"},\n","            ],\n","        }]\n","\n","        text = processor.apply_chat_template(\n","            messages,\n","            tokenize=False,\n","            add_generation_prompt=True\n","        )\n","\n","        image_inputs, video_inputs = process_vision_info(messages)\n","\n","        inputs = processor(\n","            text=[text],\n","            images=image_inputs,\n","            videos=video_inputs,\n","            padding=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        inputs = inputs.to(model.device)\n","\n","        generated_ids = model.generate(\n","            **inputs,\n","            max_new_tokens=128,\n","        )\n","\n","        generated_ids_trimmed = [\n","            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n","        ]\n","\n","        output_text = processor.batch_decode(\n","            generated_ids_trimmed,\n","            skip_special_tokens=True,\n","            clean_up_tokenization_spaces=False\n","        )[0]\n","\n","        return output_text.strip()\n","\n","    # Process chunk\n","    with open(JSONL_PATH, \"a\") as jf:\n","        for m in tqdm(chunk_to_process, desc=f\"Captioning (chunk of {len(chunk_to_process)})\"):\n","            idx = int(m[\"row_idx\"])\n","\n","            # Clear memory BEFORE processing each video\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","            try:\n","                # Try with normal settings first\n","                cap = caption_video(m[\"local_mp4\"], max_frames=32)\n","\n","            except RuntimeError as e:\n","                # If OOM or token length error, try with fewer frames\n","                if \"out of memory\" in str(e).lower() or \"token\" in str(e).lower():\n","                    tqdm.write(f\"âš ï¸  Video {idx} too large, retrying with 16 frames...\")\n","                    torch.cuda.empty_cache()\n","                    gc.collect()\n","\n","                    try:\n","                        cap = caption_video(m[\"local_mp4\"], max_frames=16)\n","                        tqdm.write(f\"âœ… Video {idx} succeeded with reduced frames\")\n","                    except Exception as e2:\n","                        # Still failed, skip this video\n","                        tqdm.write(f\"âŒ Video {idx} failed even with 16 frames: {e2}\")\n","                        skip_cnt += 1\n","                        continue\n","                else:\n","                    # Different error, skip\n","                    tqdm.write(f\"âŒ Video {idx} error: {e}\")\n","                    skip_cnt += 1\n","                    continue\n","\n","            except Exception as e:\n","                tqdm.write(f\"âŒ Video {idx} error: {e}\")\n","                skip_cnt += 1\n","                continue\n","\n","            # If we got here, we have a caption - save it!\n","            try:\n","                row = ds[idx]\n","\n","                rec = {\n","                    \"row_idx\": idx,\n","                    \"id_or_video_path\": row.get(\"path\", None),\n","                    \"video_link\": m.get(\"gif_url\", None),\n","                    \"local_mp4\": m.get(\"local_mp4\", None),\n","                    \"caption_generated\": cap,\n","                    \"labels\": row.get(\"labels\", None),\n","                    \"human_motion_caption\": row.get(\"caption\", None),\n","                    \"model\": \"chancharikm/qwen2.5-vl-7b-cam-motion\",\n","                    \"fps\": 6.0,\n","                    \"max_frames\": 32,\n","                    \"source\": \"gif_preview_converted_to_mp4\",\n","                }\n","\n","                jf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n","                jf.flush()\n","                ok_cnt += 1\n","\n","            except Exception as e:\n","                tqdm.write(f\"âŒ Failed to save video {idx}: {e}\")\n","                skip_cnt += 1\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"âœ… CHUNK COMPLETE - SAVED TO GOOGLE DRIVE!\")\n","    print(f\"{'='*70}\")\n","    print(f\"Successfully captioned this run: {ok_cnt} videos\")\n","    print(f\"Errors/skipped this run: {skip_cnt} videos\")\n","    print(f\"Total completed (all runs): {total_done + ok_cnt} / {total_videos}\")\n","    print(f\"Remaining: {total_remaining - ok_cnt} videos\")\n","    print(f\"\\nðŸ’¾ Progress permanently saved to Google Drive\")\n","\n","    if total_remaining - ok_cnt > 0:\n","        print(f\"\\nâ–¶ï¸  To continue: Run Cell 7 again\")\n","        print(f\"   (Skipped videos will be retried automatically)\")\n","    else:\n","        print(f\"\\nðŸŽ‰ ALL VIDEOS CAPTIONED! Run Cell 9 to export to Excel.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KnSq8OrwreyO"},"outputs":[],"source":["# Cell 9: Export to Excel\n","\n","from google.colab import drive\n","import pandas as pd\n","import json\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","JSONL_PATH = \"/content/camerabench/outputs/motion_captions_qwen_gif.jsonl\"\n","SAVE_XLSX = \"/content/drive/MyDrive/Deep Learning Fall 2025/motion_captions_qwen.xlsx\"\n","\n","print(\"Reading JSONL file...\")\n","rows = []\n","with open(JSONL_PATH, \"r\") as f:\n","    for line in f:\n","        s = line.strip()\n","        if not s:\n","            continue\n","        try:\n","            obj = json.loads(s)\n","            rows.append({\n","                \"row_idx\": obj.get(\"row_idx\"),\n","                \"video_link\": obj.get(\"video_link\"),\n","                \"caption_generated\": obj.get(\"caption_generated\"),\n","                \"id_or_video_path\": obj.get(\"id_or_video_path\"),\n","                \"labels\": obj.get(\"labels\"),\n","                \"human_motion_caption\": obj.get(\"human_motion_caption\"),\n","            })\n","        except Exception as e:\n","            print(f\"âš ï¸  Skipping malformed line: {e}\")\n","\n","# Create DataFrame\n","df = pd.DataFrame(rows)\n","\n","# Save to Excel\n","print(f\"Saving to Excel...\")\n","os.makedirs(os.path.dirname(SAVE_XLSX), exist_ok=True)\n","\n","with pd.ExcelWriter(SAVE_XLSX, engine=\"xlsxwriter\") as writer:\n","    df.to_excel(writer, index=False, sheet_name=\"captions\")\n","\n","    # Auto-adjust column widths\n","    worksheet = writer.sheets[\"captions\"]\n","    for i, col in enumerate(df.columns):\n","        max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2\n","        worksheet.set_column(i, i, min(max_len, 50))\n","\n","print(f\"\\n{'='*70}\")\n","print(f\"âœ… EXCEL EXPORT COMPLETE!\")\n","print(f\"{'='*70}\")\n","print(f\"Total captions: {len(df)}\")\n","print(f\"Saved to: {SAVE_XLSX}\")\n","print(f\"\\nðŸ“Š Preview:\")\n","print(df.head(3)[[\"row_idx\", \"caption_generated\"]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mo4pxZFuxZAM"},"outputs":[],"source":["# --- Colab: Progress polling & peek ---\n","!wc -l /content/camerabench/outputs/motion_captions_qwen_gif.jsonl || echo \"No JSONL yet.\"\n","\n","# Print first 3 non-empty, well-formed lines\n","import json\n","path = \"/content/camerabench/outputs/motion_captions_qwen_gif.jsonl\"\n","try:\n","    with open(path, \"r\") as f:\n","        shown = 0\n","        for line in f:\n","            s = line.strip()\n","            if not s:\n","                continue\n","            try:\n","                obj = json.loads(s)\n","                print(json.dumps(obj, indent=2, ensure_ascii=False))\n","                shown += 1\n","                if shown >= 3:\n","                    break\n","            except Exception:\n","                continue\n","except FileNotFoundError:\n","    print(\"JSONL not found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EXByFdudxaPO"},"outputs":[],"source":["# --- Colab: Export to Excel (Drive) ---\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","\n","JSONL_PATH = \"/content/camerabench/outputs/motion_captions_qwen_gif.jsonl\"\n","SAVE_XLSX = \"/content/drive/MyDrive/Deep Learning Fall 2025/scene_captions_qwen.xlsx\"\n","\n","rows = []\n","with open(JSONL_PATH, \"r\") as f:\n","    for line in f:\n","        s = line.strip()\n","        if not s:\n","            continue\n","        try:\n","            obj = json.loads(s)\n","            rows.append({\n","                \"video_link\": obj.get(\"video_link\"),\n","                \"caption_generated\": obj.get(\"caption_generated\"),\n","                \"id_or_video_path\": obj.get(\"id_or_video_path\"),\n","            })\n","        except Exception:\n","            # skip malformed lines\n","            pass\n","\n","df = pd.DataFrame(rows, columns=[\"video_link\", \"caption_generated\", \"id_or_video_path\"])\n","os.makedirs(os.path.dirname(SAVE_XLSX), exist_ok=True)\n","with pd.ExcelWriter(SAVE_XLSX, engine=\"xlsxwriter\") as writer:\n","    df.to_excel(writer, index=False, sheet_name=\"captions\")\n","\n","print(\"Saved Excel to:\", SAVE_XLSX)\n","print(df.head(3))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNYjZywo5guA5GIqhJVkDY5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"184e8eeabdf74761848572bd874f03c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1de866ce5334410195dbaac349c7c599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"334e816a82d14d52a64feff75c34a44c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_750c9b254e494841a9bc4529b13b4530","placeholder":"â€‹","style":"IPY_MODEL_e9144e12cc2e4af8b44f7abfaa0c712c","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"3b0d704ca11e4cfa9d9f7ca7b51df012":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"408b17f1477048d08610d2831501e23d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68f7d203c4794c09934326ba896f45a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_334e816a82d14d52a64feff75c34a44c","IPY_MODEL_8d174f513d534eeeb0b32c33701362e5","IPY_MODEL_7e3dea4f9698445d8c63a5e103215ae3"],"layout":"IPY_MODEL_82602070723845519c932356fbeb2686"}},"6b7c189167b042359b3366570ec147f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b98798fa56644f4830c73f0fa1c2729":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9921c108f6c84c9c9836c0dff81ed1b4","placeholder":"â€‹","style":"IPY_MODEL_6b7c189167b042359b3366570ec147f8","value":"â€‡214/214â€‡[00:00&lt;00:00,â€‡22.3kB/s]"}},"750c9b254e494841a9bc4529b13b4530":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7570cc0fba5b4fa4baa7456479c89d62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e3dea4f9698445d8c63a5e103215ae3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7570cc0fba5b4fa4baa7456479c89d62","placeholder":"â€‹","style":"IPY_MODEL_e354ea05d8a246299375c389a8694acb","value":"â€‡4/4â€‡[01:15&lt;00:00,â€‡16.73s/it]"}},"82602070723845519c932356fbeb2686":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d174f513d534eeeb0b32c33701362e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_408b17f1477048d08610d2831501e23d","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7fda273f728465cb3307a5124c38642","value":4}},"9921c108f6c84c9c9836c0dff81ed1b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f01b188f1d94023b89caf8dbe089c3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_184e8eeabdf74761848572bd874f03c5","placeholder":"â€‹","style":"IPY_MODEL_1de866ce5334410195dbaac349c7c599","value":"generation_config.json:â€‡100%"}},"9f165c083442421d8efa004fa009694f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b269698d81654592b43ee4d29d1d66b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f01b188f1d94023b89caf8dbe089c3f","IPY_MODEL_b730aebeb7b44b1787a71fff81c417f1","IPY_MODEL_6b98798fa56644f4830c73f0fa1c2729"],"layout":"IPY_MODEL_3b0d704ca11e4cfa9d9f7ca7b51df012"}},"b730aebeb7b44b1787a71fff81c417f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8aed84793fa4fe79d566798956ecba3","max":214,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f165c083442421d8efa004fa009694f","value":214}},"b7fda273f728465cb3307a5124c38642":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e354ea05d8a246299375c389a8694acb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8aed84793fa4fe79d566798956ecba3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9144e12cc2e4af8b44f7abfaa0c712c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}